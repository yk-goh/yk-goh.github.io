# 문제: 교재의 비선형 SVM 모형(181쪽)에서 kernel='rbf', 'poly', 'sigmoid'로 변경하고 c와 gamma를 변경하여 accuracy와 과대적합에 미치는 영향을 논의하는 예제
> 비선형 SVM에서 gamma가 클수록 분류선의 비선형성이 강해지므로 train 데이터에 매우 유연하게 적합되어 overfitting이 발생한다
> - 비선형 SVM에서 kernel='rbf'이면 gamma가 클수록 분산이 낮은 작은 bell-shape의 정규분포모형의 비선형 적합이 된다. 반대로 gamma가 작을수록 분산이 큰 정규분포 모형의 비선형 적합이 된다. (넓게 퍼져있는 정규분포이므로 관측치는 거의 선형과 가까움) 분산이 매우 큰 정규분포는 거의 선형이므로 실질적으로 rbf는 선형과 같아진다
> - poly와 sigmoid는 gamma가 클수록 비선형성이 강해진다


<br/>
클래스의 분류선이 비선형인 경우, 차원을 증가시킴으로써 분류를 쉽게 할 수 있다. 그런데 이미 고차원인 데이터의 경우, 차원을 증가시키면 데이터의 밀도가 낮아져 모델이 성능을 발휘하지 못하는 ‘차원의 저주’ 문제가 발생하기 쉽다. 이럴 때 커널함수가 유용하게 사용된다. 왜냐하면 커널함수는 데이터의 차원을 실제로 증가시키지는 않으면서 고차원에서의 데이터 포인트 간 거리 혹은 유사도를 계산하기 때문이다(이를 커널 속임수kernel trick이라 한다).

주로 사용되는 커널함수인 rbf, poly 그리고 sigmoid는 γ를 포함하는데, γ는 개별 관측치의 영향력을 제어하는 초모수이다. 


## 1. rbf 커널함수
radial basis function(rbf)는 특정 관측치를 기준점으로 지정하고 각 데이터 포인트와 기준점의 거리(유사도)를 측정하며 이를 바탕으로 경계를 결정하는 방식이다. 

rbf에서 γ는 개별 데이터 포인트의 영향력의 범위를 의미한다. 특히 서포트벡터와 관련하여 영향력 반지름이 중요한데, 이유는 최적의 분류선($f(X)=\beta_0+\beta^TX$)을 찾는 문제에 있어 계산에 사용되는 데이터 포인트가 서포트벡터이기 때문이다. rbf 커널함수는 γ가 클수록 분산이 작은 정규분포 형태의 비선형 모형이 된다. γ가 너무 크면 서포트벡터의 영향력 반지름이 서포트벡터 자체에만 국한되면서 구불구불한 모양의 결정 경계를 만든다. 이 경우 C를 조절하여도 과적합을 방지하기 어렵다.
반대로 γ가 작을수록 분산이 큰 정규분포 형태의 비선형 모형이 된다. γ가 매우 작아 분산이 매우 크면 정규분포는 거의 선형이므로 rbf 커널함수는 실질적으로 선형모형과 같아진다. 이 경우 데이터의 패턴을 파악할 수 없다. 모든 서포트벡터의 영향력 반지름에 train data 전체가 들어가기 때문이다. 



```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

def gaussian_rbf(x, c, gamma):
    return np.exp(-gamma * np.linalg.norm(x - c)**2)

# Define parameters
gamma_001 = .01
gamma_01 = .1
gamma_1 = 1
gamma_10 = 10
c = 0  # Center of the RBF

# Generate input values
x_values = np.linspace(-3, 3, 100)

# Calculate RBF values for each input
rbf_values_001 = [gaussian_rbf(x, c, gamma_001) for x in x_values]
rbf_values_01 = [gaussian_rbf(x, c, gamma_01) for x in x_values]
rbf_values_1 = [gaussian_rbf(x, c, gamma_1) for x in x_values]
rbf_values_10 = [gaussian_rbf(x, c, gamma_10) for x in x_values]

# Plot the RBF
plt.plot(x_values, rbf_values_001, label='gamma=.01')
plt.plot(x_values, rbf_values_01, label='gamma=.1')
plt.plot(x_values, rbf_values_1, label='gamma=1')
plt.plot(x_values, rbf_values_10, label='gamma=10')
plt.title('Radial Basis Function (Gaussian)')
plt.xlabel('Input')
plt.ylabel('RBF Value')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

```


    
![png](/images/m1/a8_2/output_3_0.png)
    


iris 데이터에서 두 개의 특성변수(sepal_length, sepal_width)만 이용하여 C=1로 고정한 뒤 γ값을 변경하며 SVM을 적합해 플롯을 그리면 다음과 같다. γ 값이 작을 때 분류 경계가 직선이며, γ 값이 클수록 분류 경계의 비선형성이 강해지다가 γ가 매우 크면 분류 경계에 해당하는 반원이 데이터포인트 주변에만 작게 생성되는 것을 볼 수 있다. 


```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.svm import SVC
import numpy as np

iris = load_iris()
X = iris.data[:, [0, 1]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))


def plot_iris(X, y, model, title, xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5):
    XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000),
                         np.arange(ymin, ymax, (ymax-ymin)/1000))
    ZZ = np.reshape(model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape)
    plt.contourf(XX, YY, ZZ, cmap=plt.cm.Pastel1, alpha=.8)
    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='^', label='0', s=10)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='g', marker='o', label='1', s=10)
    plt.scatter(X[y == 2, 0], X[y == 2, 1], c='b', marker='s', label='2', s=10)
    plt.xlim(xmin, xmax)
    plt.ylim(ymin, ymax)
    plt.xlabel("sepal_length")
    plt.ylabel("sepal_width")
    plt.title(title)


# model1 = SVC(kernel='linear').fit(X_test_std, y_test)
model1 = SVC(kernel='rbf', random_state=0,
             gamma=0.01, C=1.0).fit(X_test_std, y_test)
model2 = SVC(kernel='rbf', random_state=0,
             gamma=1, C=1.0).fit(X_test_std, y_test)
model3 = SVC(kernel='rbf', random_state=0, 
             gamma=100, C=1.0).fit(X_test_std, y_test)

plt.figure(figsize=(12, 4))
plt.subplot(131)
plot_iris(X_test_std, y_test, model1, "kernel=rbf, gamma=0.01")
plt.subplot(132)
plot_iris(X_test_std, y_test, model2, "kernel=rbf, gamma=1")
plt.subplot(133)
plot_iris(X_test_std, y_test, model3, "kernel=rbf, gamma=100")
plt.tight_layout()
plt.show()
```


    
![png](/images/m1/a8_2/output_5_0.png)
    


## 2. poly, sigmoid 커널함수

아래는 iris 데이터셋에 커널함수를 rbf, poly, sigmoid로 한 비선형 SVM을 적용하고 γ와 C를 변경하며 train set과 test set에서의 accuracy를 확인한 결과이다. 
-	γ를 크게 주었을 때(γ=100), C(허용하는 오차 크기와 반비례) 값에 관계없이 train set에 과대적합 되었으며 test set에서의 성능이 나쁨을 볼 수 있다
-	γ를 작게 주었을 때(γ=0.01), C값 또한 작으면(허용하는 오차의 크기가 크면) accuracy가 낮은 underfit이 발생함을 확인할 수 있다

요약하면, rbf 커널함수, p차 다항식 커널함수, 시그모이드 함수 모두 γ가 커질수록 분류선의 비선형성이 강해져 train 데이터에 유연하게 적합하므로(즉, variance가 크다) overfitting이 발생한다.


```python
from sklearn import metrics
from itertools import product

kernel_list = ['rbf', 'poly', 'sigmoid']
gamma_list = [0.01, 1, 100]
C_list = [0.1, 1, 10, 100]

prod = product(kernel_list, gamma_list, C_list)

for kernel, gamma, C in prod:
    ksvm = SVC(kernel=kernel, C=C, gamma=gamma, random_state=42)
    ksvm.fit(X_train, y_train)
    y_train_pred = ksvm.predict(X_train)
    y_test_pred = ksvm.predict(X_test)
    print('\n-----kernel = {}, gamma = {}, C = {}-----'.format(kernel, gamma, C))
    print('train accuracy: ',metrics.accuracy_score(y_train, y_train_pred).round(2))
    print('test accuracy: ',metrics.accuracy_score(y_test, y_test_pred).round(2)) 

```

    
    -----kernel = rbf, gamma = 0.01, C = 0.1-----
    train accuracy:  0.93
    test accuracy:  0.93
    
    -----kernel = rbf, gamma = 0.01, C = 1-----
    train accuracy:  0.95
    test accuracy:  0.93
    
    -----kernel = rbf, gamma = 0.01, C = 10-----
    train accuracy:  0.98
    test accuracy:  0.96
    
    -----kernel = rbf, gamma = 0.01, C = 100-----
    train accuracy:  0.99
    test accuracy:  0.98
    
    -----kernel = rbf, gamma = 1, C = 0.1-----
    train accuracy:  0.96
    test accuracy:  0.96
    
    -----kernel = rbf, gamma = 1, C = 1-----
    train accuracy:  0.98
    test accuracy:  0.96
    
    -----kernel = rbf, gamma = 1, C = 10-----
    train accuracy:  0.99
    test accuracy:  0.96
    
    -----kernel = rbf, gamma = 1, C = 100-----
    train accuracy:  1.0
    test accuracy:  0.91
    
    -----kernel = rbf, gamma = 100, C = 0.1-----
    train accuracy:  1.0
    test accuracy:  0.6
    
    -----kernel = rbf, gamma = 100, C = 1-----
    train accuracy:  1.0
    test accuracy:  0.67
    
    -----kernel = rbf, gamma = 100, C = 10-----
    train accuracy:  1.0
    test accuracy:  0.64
    
    -----kernel = rbf, gamma = 100, C = 100-----
    train accuracy:  1.0
    test accuracy:  0.64
    
    -----kernel = poly, gamma = 0.01, C = 0.1-----
    train accuracy:  0.77
    test accuracy:  0.76
    
    -----kernel = poly, gamma = 0.01, C = 1-----
    train accuracy:  0.89
    test accuracy:  0.84
    
    -----kernel = poly, gamma = 0.01, C = 10-----
    train accuracy:  0.98
    test accuracy:  0.96
    
    -----kernel = poly, gamma = 0.01, C = 100-----
    train accuracy:  0.99
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 1, C = 0.1-----
    train accuracy:  0.99
    test accuracy:  0.93
    
    -----kernel = poly, gamma = 1, C = 1-----
    train accuracy:  0.99
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 1, C = 10-----
    train accuracy:  1.0
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 1, C = 100-----
    train accuracy:  1.0
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 100, C = 0.1-----
    train accuracy:  1.0
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 100, C = 1-----
    train accuracy:  1.0
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 100, C = 10-----
    train accuracy:  1.0
    test accuracy:  0.98
    
    -----kernel = poly, gamma = 100, C = 100-----
    train accuracy:  1.0
    test accuracy:  0.98
    
    -----kernel = sigmoid, gamma = 0.01, C = 0.1-----
    train accuracy:  0.86
    test accuracy:  0.78
    
    -----kernel = sigmoid, gamma = 0.01, C = 1-----
    train accuracy:  0.86
    test accuracy:  0.78
    
    -----kernel = sigmoid, gamma = 0.01, C = 10-----
    train accuracy:  0.97
    test accuracy:  0.91
    
    -----kernel = sigmoid, gamma = 0.01, C = 100-----
    train accuracy:  0.91
    test accuracy:  0.8
    
    -----kernel = sigmoid, gamma = 1, C = 0.1-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 1, C = 1-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 1, C = 10-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 1, C = 100-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 100, C = 0.1-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 100, C = 1-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 100, C = 10-----
    train accuracy:  0.33
    test accuracy:  0.33
    
    -----kernel = sigmoid, gamma = 100, C = 100-----
    train accuracy:  0.33
    test accuracy:  0.33



