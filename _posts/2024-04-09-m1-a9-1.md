---
layout: single
title: "comparison between MDS, LLE, Isomap and t-SNE"
categories: ML
sidebar: true
use_math: true
---
# 문제: wine_data를 이용하여 MDS, LLE, Isomap, t-SNE의 성능 비교
> - 차원축소의 목적은 특성변수의 차원을 축소하여 새로운 특성변수를 만드는 것이다. 즉, 차원축소는 **unsupervised learning** 기법이다. 그러나 차원축소된 새로운 특성변수는 차원축소 이전의 특성변수가 가지고 있는 정보를 가능한 한 유지해야 한다. 그러므로 차원축소는 전체 데이터(train set, test set 구분 없이)에 차원축소기법을 적용해야 한다. 이런 이유로 MDS와 t-sne는 test data에 대한 transform 함수가 제공되지 않는다.
> - 차원축소 모형을 적용할 때 train set과 test set에 각각 fit_transform을 적용하면 두 데이터셋에 다른 모형을 적용한 것이 된다. 이렇게 하면 안된다!
> - 차원축소기법을 <span style="color:pink">전체 데이터</span>에 적용하여 차원축소된 특성변수를 가지고 특정 모형, 예를 들어 logistic regression을 돌린 후, 이 결과를 원래의 특성변수로 동일한 모형을 적용하여 두 결과를 비교해야 한다. 즉, 원래의 특성변수로 구현한 모형의 성능에 가장 근접한 결과를 보이는 것이 가장 우수한 차원축소 기법이다.
> - 물론 차원축소 기법의 초모수는 최대한 원래의 모형(원래의 특성변수)의 성능에 가깝도록 조율해야 한다. 
---


```python
# wine data 불러오기 
import pandas as pd
dat_wine=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/'
                     'wine/wine.data',header=None) # wine data 불러오기
# X, y: 전체 데이터
X,y=dat_wine.iloc[:,1:].values, dat_wine.iloc[:,0].values

```

## 0. 원래의 특성변수에 Logistic Regression을 적용한 결과
먼저 원래의 특성변수(13개)를 사용하여 Logistic Regression을 적합한다. Train set에서의 accuracy는 1.0, test set에서의 accuracy는 0.981로 약간의 과대적합이 의심되나 accuracy가 상당히 높다.


```python
# X_train, X_test, y_train, y_test: 분할된 데이터 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1, stratify=y)

# 표준화 
from sklearn.preprocessing import StandardScaler
std=StandardScaler()
# Fit the scaler on the training set to compute the scaling parameters
std.fit(X_train)
# Scale the training set using the fitted scaler
X_train_std=std.transform(X_train)
# Scale the test set using the same scaling parameters obtained from the training set
X_test_std=std.transform(X_test)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train_std, y_train)

print('Training accuracy', lr.score(X_train_std, y_train))
print('Test accuracy', lr.score(X_test_std, y_test))
```

    Training accuracy 1.0
    Test accuracy 0.9814814814814815


# 1. MDS
다양한 차원축소 기법을 비교하기 위해 축소된 차원의 개수(n_components)를 2로 고정한다.

전체 X(특성변수) 데이터를 표준화 하고 MDS를 적합-변환한다(fit-transform). 이 변환된(차원축소 된) 데이터는 n_components=2로 지정하였으므로 2개 차원을 갖는다. 차원 축소된 데이터를 train set과 test set으로 나누고, train set에 로지스틱 회귀를 적합한다. score() 메서드를 사용하여 train set과 test set의 accuracy를 얻는다. test accuracy=0.944로, 차원축소 전 원래 특성변수에 로지스틱 회귀를 적합한 모형보다 정확도가 떨어짐을 확인할 수 있다. 


```python
# 표준화
X_std = std.transform(X)

# 차원축소
from sklearn.manifold import MDS
from sklearn.metrics import accuracy_score

mds = MDS(n_components=2)
X_std_mds = mds.fit_transform(X_std)

# train-test split
X_std_mds_train, X_std_mds_test, y_train, y_test = train_test_split(X_std_mds, y, test_size=0.3, random_state=1, stratify=y)

# 차원 축소된 train dataset에 로지스틱 회귀 적합 
lr_mds = LogisticRegression()
lr_mds.fit(X_std_mds_train, y_train)

# train, test accuracy
print('Training accuracy: ', lr_mds.score(X_std_mds_train, y_train).round(3))
print('Test accuracy: ', lr_mds.score(X_std_mds_test, y_test).round(3))

```

    Training accuracy:  0.968
    Test accuracy:  0.944


    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.
      warnings.warn(


# 2. LLE
LLE(Locally Linear Embedding) 또한 위와 같은 방법으로 진행한다. 단, LLE는 모든 데이터 포인트를 n개의 주변 데이터의 선형결합으로 근사할 수 있다는 가정에서 출발하므로 최적의 n_neighbors 파라미터 값을 찾아야 한다. 이를 위해 GridSearchCV() 함수를 사용하고, 전체 데이터의 관측치 수가 178개이므로 n_neighbors를 찾는 범위를 178보다 작게 지정한다. cross-validation 결과 최적의 n_neighbors=5로 나타났다. 표준화된(StandardScaler 적용) X(특성변수) 전체 데이터에 LocallyLinearEmbedding()을 적용하며, 앞서 구한 n_neighbors와 2로 고정된 n_component를 지정하여 차원 축소된 데이터를 만든다. 차원 축소된 데이터를 train set과 test set으로 분할하고 train set에 로지스틱 회귀를 적합, score() 메서드로 train set과 test set의 accuracy를 구하면 다음과 같다. 


```python
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
import numpy as np
from sklearn.manifold import LocallyLinearEmbedding

# 파이프 만들기
pipe_lle = make_pipeline(LocallyLinearEmbedding(), LogisticRegression())

# 최상의 초모수 탐색
param_grid = [{'locallylinearembedding__n_neighbors': np.arange(2, 100, 1)}]

gs = GridSearchCV(estimator= pipe_lle, param_grid = param_grid, scoring='accuracy', cv=5)
gs = gs.fit(X_std, y)

print(gs.best_score_)
print(gs.best_params_)

```

    0.9550793650793651
    {'locallylinearembedding__n_neighbors': 5}



```python
# 차원 축소
best_n_neighbors = gs.best_params_['locallylinearembedding__n_neighbors']
lle = LocallyLinearEmbedding(n_neighbors=best_n_neighbors, n_components=2)

X_std_lle = lle.fit_transform(X_std)

# train-test split
X_std_lle_train, X_std_lle_test, y_train, y_test = train_test_split(X_std_lle, y, test_size=0.3, random_state=1, stratify=y)

# 차원 축소된 train dataset에 로지스틱 회귀 적합
lr_lle = LogisticRegression()
lr_lle.fit(X_std_lle_train, y_train)

# train, test accuracy 
print('Training accuracy: ', lr_lle.score(X_std_lle_train, y_train).round(3))
print('Test accuracy: ', lr_lle.score(X_std_lle_test, y_test).round(3))

```

    Training accuracy:  0.895
    Test accuracy:  0.944


# 3. Isomap
Isomap 또한 LLE와 유사하게 데이터 포인트 각각의 K-Nearest Neighbors를 구하고 점 간의 Euclidean distance를 구하는 방식이다. Isomap 또한 2.와 같은 방식으로 차원축소 된 데이터의 train, test accuracy를 구하면 다음과 같다. test accuracy가 다른 차원축소 기법을 적용했을 때보다 높게 나타났다. 


```python
from sklearn.manifold import Isomap

# 파이프 만들기
pipe_iso = make_pipeline(Isomap(), LogisticRegression())

# 최상의 초모수 탐색
param_grid = [{'isomap__n_neighbors': np.arange(2, 100, 1)}]

gs = GridSearchCV(estimator= pipe_iso, param_grid = param_grid, scoring='accuracy', cv=5)
gs = gs.fit(X_std, y)

print(gs.best_score_)
print(gs.best_params_)
```

    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/sklearn/manifold/_isomap.py:373: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.
      self._fit_transform(X)
    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/scipy/sparse/_index.py:102: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.
      self._set_intXint(row, col, x.flat[0])
    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/sklearn/manifold/_isomap.py:373: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.
      self._fit_transform(X)
    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/scipy/sparse/_index.py:102: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.
      self._set_intXint(row, col, x.flat[0])
    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/sklearn/manifold/_isomap.py:373: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.
      self._fit_transform(X)
    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/scipy/sparse/_index.py:102: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.
      self._set_intXint(row, col, x.flat[0])


    0.9661904761904762
    {'isomap__n_neighbors': 76}



```python
# 차원 축소
best_n_neighbors = gs.best_params_['isomap__n_neighbors']
iso = Isomap(n_neighbors=best_n_neighbors, n_components=2)

X_std_iso = iso.fit_transform(X_std)

# train-test split
X_std_iso_train, X_std_iso_test, y_train, y_test = train_test_split(X_std_iso, y, test_size=0.3, random_state=1, stratify=y)

# 차원 축소된 train dataset에 로지스틱 회귀 적합
lr_iso = LogisticRegression()
lr_iso.fit(X_std_iso_train, y_train)

# train, test accuracy 
print('Training accuracy: ', lr_iso.score(X_std_iso_train, y_train).round(3))
print('Test accuracy: ', lr_iso.score(X_std_iso_test, y_test).round(3))
```

    Training accuracy:  0.952
    Test accuracy:  0.981


# 4. t-SNE
t-SNE는 원래 공간에서 가까이 있는 점들을 차원 축소 된 공간에서도 가깝게 배치하면서(locality) 동시에 원래 공간에서 전체적으로 점들이 배치된 구조(globality) 또한 차원 축소 된 공간에서 보존하는 것을 목표로 하는 차원축소 기법이다. 이를 위해서는 가까운 점 뿐 아니라 멀리 있는 점의 위치도 고려해야 하는데, 이러한 trade-off는 TSNE() 함수의 perplexity 값에 영향을 받는다. 따라서 perplexity를 결정하기 위해 각각의 perplexity로 표준화된 전체 X 데이터를 차원축소 하고, 차원축소 된 데이터에 로지스틱 회귀를 적용하여 cross-validation을 진행하며 가장 높은 평균 accuracy를 보이는 perplexity를 채택한다. 다시 TSNE() 함수에서 perplexity를 최적의 perplexity(=45)로 지정하여 특성변수를 차원 축소하고, 차원축소 된 데이터를 train set과 test set으로 분할하여 train set에 로지스틱 회귀를 적합, train, test accuracy를 구하면 다음과 같다. 


```python
from sklearn.manifold import TSNE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
import numpy as np

# Define the perplexity values to try
perplexities = np.arange(2, 50, 1)

# Dictionary to store perplexity and corresponding accuracy
perplexity_accuracy = {}

# Iterate over perplexity values
for perplexity in perplexities:
    # Create t-SNE model with current perplexity
    tsne = TSNE(perplexity=perplexity)
    
    # Fit t-SNE model and transform the data
    X_tsne = tsne.fit_transform(X_std)
    
    # Define logistic regression model
    lr = LogisticRegression()
    
    # Calculate cross-validation accuracy for logistic regression
    accuracy = cross_val_score(lr, X_tsne, y, cv=5, scoring='accuracy').mean()
    
    # Store perplexity and corresponding accuracy in dictionary
    perplexity_accuracy[perplexity] = accuracy

# Find the perplexity with the highest accuracy
best_perplexity = max(perplexity_accuracy, key=perplexity_accuracy.get)
best_accuracy = perplexity_accuracy[best_perplexity]

print("Best Perplexity:", best_perplexity)
print("Best Accuracy:", best_accuracy)

```

    Best Perplexity: 45
    Best Accuracy: 0.9777777777777779



```python
# 차원 축소
tsne = TSNE(perplexity=best_perplexity)

X_std_tsne = tsne.fit_transform(X_std)

# train-test split
X_std_tsne_train, X_std_tsne_test, y_train, y_test = train_test_split(X_std_tsne, y, test_size=0.3, random_state=0, stratify=y)

# 차원 축소된 train dataset에 로지스틱 회귀 적합
lr_tsne = LogisticRegression()
lr_tsne.fit(X_std_tsne_train, y_train)

# train, test accuracy 
print('Training accuracy: ', lr_tsne.score(X_std_tsne_train, y_train).round(3))
print('Test accuracy: ', lr_tsne.score(X_std_tsne_test, y_test).round(3))
```

    Training accuracy:  0.968
    Test accuracy:  0.981


여러가지 차원축소 기법을 비교한 바, 원본 데이터에 로지스틱 회귀를 적합한 성능과 가장 유사한 성능을 보인 것은 Isomap 및 t-SNE로 차원 축소 된 데이터에 로지스틱 회귀를 적합한 경우였으므로 wine_data의 원래 특성변수의 정보를 가장 잘 보존한 차원 축소 기법은 Isomap과 t-SNE라고 결론 내린다. 
