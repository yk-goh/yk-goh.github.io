---
layout: single
title: "LIME and SP-LIME"
categories: XAI
sidebar: true
use_math: true
---


# 문제: housing data에 Lime과 SP-Lime 적용


```python
import pandas as pd
import numpy as np 
import lime 
import matplotlib.pyplot as plt 
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
import warnings 
warnings.filterwarnings('ignore')
```

## 1. 데이터 확인
housing_data.csv 파일을 불러들이고 .head()와 .describe() 메서드로 데이터를 확인한다. 총 12개의 실수형 특성변수와 1개의 더미 변수(‘CHAS’)가 있고, 목적변수로 ‘MEDV’가 있다. 


```python
boston = pd.read_csv('housing_data.csv', index_col=0)
print(boston.shape)
boston.head()
```

    (506, 14)





<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>




```python
boston.describe()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
      <td>506.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.613524</td>
      <td>11.363636</td>
      <td>11.136779</td>
      <td>0.069170</td>
      <td>0.554695</td>
      <td>6.284634</td>
      <td>68.574901</td>
      <td>3.795043</td>
      <td>9.549407</td>
      <td>408.237154</td>
      <td>18.455534</td>
      <td>356.674032</td>
      <td>12.653063</td>
      <td>22.532806</td>
    </tr>
    <tr>
      <th>std</th>
      <td>8.601545</td>
      <td>23.322453</td>
      <td>6.860353</td>
      <td>0.253994</td>
      <td>0.115878</td>
      <td>0.702617</td>
      <td>28.148861</td>
      <td>2.105710</td>
      <td>8.707259</td>
      <td>168.537116</td>
      <td>2.164946</td>
      <td>91.294864</td>
      <td>7.141062</td>
      <td>9.197104</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.006320</td>
      <td>0.000000</td>
      <td>0.460000</td>
      <td>0.000000</td>
      <td>0.385000</td>
      <td>3.561000</td>
      <td>2.900000</td>
      <td>1.129600</td>
      <td>1.000000</td>
      <td>187.000000</td>
      <td>12.600000</td>
      <td>0.320000</td>
      <td>1.730000</td>
      <td>5.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.082045</td>
      <td>0.000000</td>
      <td>5.190000</td>
      <td>0.000000</td>
      <td>0.449000</td>
      <td>5.885500</td>
      <td>45.025000</td>
      <td>2.100175</td>
      <td>4.000000</td>
      <td>279.000000</td>
      <td>17.400000</td>
      <td>375.377500</td>
      <td>6.950000</td>
      <td>17.025000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.256510</td>
      <td>0.000000</td>
      <td>9.690000</td>
      <td>0.000000</td>
      <td>0.538000</td>
      <td>6.208500</td>
      <td>77.500000</td>
      <td>3.207450</td>
      <td>5.000000</td>
      <td>330.000000</td>
      <td>19.050000</td>
      <td>391.440000</td>
      <td>11.360000</td>
      <td>21.200000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3.677083</td>
      <td>12.500000</td>
      <td>18.100000</td>
      <td>0.000000</td>
      <td>0.624000</td>
      <td>6.623500</td>
      <td>94.075000</td>
      <td>5.188425</td>
      <td>24.000000</td>
      <td>666.000000</td>
      <td>20.200000</td>
      <td>396.225000</td>
      <td>16.955000</td>
      <td>25.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>88.976200</td>
      <td>100.000000</td>
      <td>27.740000</td>
      <td>1.000000</td>
      <td>0.871000</td>
      <td>8.780000</td>
      <td>100.000000</td>
      <td>12.126500</td>
      <td>24.000000</td>
      <td>711.000000</td>
      <td>22.000000</td>
      <td>396.900000</td>
      <td>37.970000</td>
      <td>50.000000</td>
    </tr>
  </tbody>
</table>
</div>



한편 구역의 주택가격 중앙값을 뜻하는 목적변수(‘MEDV’)의 히스토그램을 그려보면, 다음과 같이 15(단위: $1000)와 25(단위: $1000) 사이에 가장 많이 분포함을 알 수 있다. 


```python
plt.hist(boston['MEDV'], bins=20, color='darkslateblue')
```




    (array([ 9., 12., 18., 37., 40., 42., 83., 71., 72., 12., 23., 18., 16.,
            14.,  7.,  1.,  5.,  5.,  2., 19.]),
     array([ 5.  ,  7.25,  9.5 , 11.75, 14.  , 16.25, 18.5 , 20.75, 23.  ,
            25.25, 27.5 , 29.75, 32.  , 34.25, 36.5 , 38.75, 41.  , 43.25,
            45.5 , 47.75, 50.  ]),
     <BarContainer object of 20 artists>)




    
![png](/images/m4/a2_1/output_6_1.png)
    


## 2. 전처리
boston housing 데이터셋을 numpy 형으로 변환하고 학습데이터와 시험데이터로 분할한다. 


```python
from sklearn.model_selection import train_test_split

# 목적변수
target_name = 'MEDV'
# 전체 특성변수 
feature_names = boston.columns.to_list()
feature_names.remove(target_name)

X = boston[feature_names].to_numpy()
y = boston[target_name].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=123, shuffle=True)

print(X_train.shape)
print(X_test.shape) 
```

    (455, 13)
    (51, 13)


## 3. Light GBM 회귀모형 적합
1.의 요약통계량을 보면 특성변수 간 범위 차이가 크기 때문에 StandardScaler 클래스를 사용하여 특성변수를 표준화 할 수도 있다. 그러나 과제에서 사용할 블랙박스 모형이 트리(의사결정나무) 기반의 Light GBM으로, 변수의 스케일에 민감하지 않기 때문에 표준화 없이 Light GBM 모형에 적합하기로 한다. 

회귀나무 모형인 LGBMRegressor는 결정계수 R^2을 반환한다. 시험데이터의 R^2=0.72로 과대적합이 일어났다. 초모수를 조절하여 과대적합을 해소할 필요가 있다. 


```python
from lightgbm import LGBMRegressor 

lgb = LGBMRegressor()
lgb.fit(X_train, y_train)

print('Test R^2 score: ', lgb.score(X_test, y_test))
print('Train R^2 score: ', lgb.score(X_train, y_train))
```

    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000659 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 1142
    [LightGBM] [Info] Number of data points in the train set: 455, number of used features: 13
    [LightGBM] [Info] Start training from score 22.247253
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    ...
    Test R^2 score:  0.7249485253146986
    Train R^2 score:  0.979442670706155


## 4. Lime 적용
Boston 데이터가 Tabular이므로 LimeTabularExplainer를 호출한다. Tabular 데이터로부터 로컬 특성변수 중요도를 구하기 위해 합성데이터를 생성할 때 블랙박스 모형(Light GBM)을 학습시킨 데이터로부터 각 특성변수의 평균과 분산을 계산해, 이 평균과 분산을 가진 정규분포로부터 특성변수를 임의추출한다. 따라서 LimeTabularExplainer를 인스턴스화 할 때 학습데이터(X_train)를 첫 번째 파라미터로 넣어준다. 한편 회귀가 목적이므로 mode=’regression’으로 지정한다.  


```python
from lime.lime_tabular import LimeTabularExplainer 
explainer = LimeTabularExplainer(X_train, 
                                mode = "regression", 
                                feature_names=feature_names)
```

아래 프로그램은 관심표본을 시험데이터의 인덱스 2 표본으로 지정하여 블랙박스 모형의 예측값(48.08)과 실제값(45.4)을 출력하고 있다. 그리고 explain_instance 메서드를 사용하여 관심표본의 lime 특성변수 중요도를 구한다. 이 때, 회귀모형이므로 예측함수(predict_fn=) 파라미터에는 lgb.predict를 지정한다. 


```python
idx = 2
print("Prediction: ", lgb.predict(X_test[idx].reshape(1, -1)))
print("Actual: ", y_test[idx])

explanation = explainer.explain_instance(X_test[idx], 
                                        predict_fn = lgb.predict, 
                                        num_features=5)
explanation.show_in_notebook()
```


```python
print("Explanation Local Prediction: ", explanation.local_pred)
print("Explanation Global Prediction: ", explanation.predicted_value)
```

    Explanation Local Prediction:  [37.42331013]
    Explanation Global Prediction:  48.079441622426145

![png](/images/m4/a2_1/lime.png)

시각화 결과는 크게 세 부분으로 이루어져 있는데, 가장 왼쪽은 블랙박스 모형에 의한 예측치를 보여준다. 원래 모형에 의한 예측치(48.08)는 대리모형에 의한 예측치(37.42)보다 작다. 가운데 막대 그래프를 살펴보면, 관심표본에 대하여 RM(방 개수), LSTAT(하위 계층의 비율), PTRATIO(학생/교사 비율), DIS(업무지구까지의 거리), TAX(재산세율) 순으로 특성변수 중요도를 가지고 있으며 각각 9.78, 7.01, 1.31, 1.19, 0.66의 중요도를 가짐을 알 수 있다. 이 중 주황색 특성변수는 예측치에 양의 기여를, 파란색 특성변수는 예측치에 음의 기여를 한다. 관심표본에서 중요도가 높은 특성변수 값이 주택가격 예측치에 주로 양의 기여를 하고 있다. 한편, 막대그래프 위의 부등호와 숫자는 의사결정나무의 노드를 의미한다. 우측의 Feature와 Value로 구성된 표는 관심표본의 특성변수 값이며, 각 특성변수 값은 가운데 막대그래프에 표기된 노드값 부등호를 만족하는 것을 확인할 수 있다. 예를 들어 관심표본의 RM 특성변수 값은 7.82이며, 이는 positive에 해당하기 위한 노드값인 RM>6.61을 만족한다. 

관심표본의 실제 주택가격 중앙값(MEDV)은 45.4로 Step 1의 요약통계량과 히스토그램에 비추어 볼 때 표본들 중 상당히 높은 편에 속한다. 이에 대하여, 해당 표본(거주구역)의 (1) 평균 방 개수가 많고 (2) 하위 계층(빈곤층) 비율이 낮으며 (3) 교사 당 학생 수가 적은 편(좋은 학군이라 말할 수 있다)이라는 점이 큰 설명력을 갖는다. 


## 5. SP-LIME 적용
다음으로 SP-LIME을 이용하여 블랙박스 모형에서 특성변수 중요도 측면 대표성을 갖는 표본을 추출한다. SP-LIME은 특성변수의 중요도가 큰 표본들로 구성하되, 이 표본들이 모든 특성변수들을 대표하도록 동일한 특성변수를 대표하는 표본은 중복하여 포함하지 않는다. 

아래 프로그램은 특성변수 중요도 측면에서 대표성을 갖는 3개의 표본을 학습데이터로부터 추출하고 예측값과 특성변수 중요도를 보여주고 있다. 13개의 특성변수가 모두 포함되어 있음을 확인할 수 있다. 


```python
from lime import submodular_pick

sp_obj = submodular_pick.SubmodularPick(explainer=explainer, 
                                        data = X_train, 
                                        predict_fn=lgb.predict, 
                                        num_features=10, 
                                        num_exps_desired=3)
[exp.show_in_notebook() for exp in sp_obj.sp_explanations]
```


![png](/images/m4/a2_1/splime1.png)


![png](/images/m4/a2_1/splime2.png)


![png](/images/m4/a2_1/splime3.png)

