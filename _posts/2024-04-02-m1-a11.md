---
layout: single
title: "Linear Regression, SVM, and RANSAC on House Data"
categories: ML
sidebar: true
use_math: true
---

# 문제: house 데이터에 적용한 linear regression, 선형 SVM, 그리고 RANSAC에 의해 추정된 선형모형을 제시하고, RANSAC(outlier가 제거된 OLS 선형모형)과의 비교를 통해 linear regression 모형과 SVM을 설명한다
> - RANSAC은 robust 회귀이므로 RANSAC의 회귀계수와 linear regression의 회귀계수를 비교하면 어떤 특성변수에 outlier가 있는지 확인할 수 있다.
    - 회귀계수의 차이가 큰 특성변수에 outlier가 있다
> - 마찬가지로 RANSAC 회귀계수와 선형 SVM 회귀계수를 비교하여 어떤 특성변수가 outlier의 영향을 크게 받았는지 유추할 수 있다
> - RANSAC-SVM의 회귀계수 차이가 RANSAC-linear의 회귀계수 차이보다 작으므로, **SVM이 linear보다 outlier에 대한 저항성이 크다고 할 수 있다**
> - outlier가 있으면 MSE 비교는 큰 의미가 없다. 단지, training 데이터의 MSE에 비해 test 데이터의 MSE가 지나치게 크면 linear regression에서는 outlier를 의심해야 한다. RANSAC도 같은 현상이 일어날 수 있다. 데이터 크기가 작으면 RANSAC의 성능이 좋지 않기 때문이다.

## 0. 데이터 확인과 전처리


```python
import pandas as pd
house = pd.read_csv('./housing.csv', header=None, sep='\s+')
house.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
house.head()

""" 변수 설명 """
# CRIM - per capita crime rate by town
# ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
# INDUS - proportion of non-retail business acres per town.
# CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
# NOX - nitric oxides concentration (parts per 10 million)
# RM - average number of rooms per dwelling
# AGE - proportion of owner-occupied units built prior to 1940
# DIS - weighted distances to five Boston employment centres
# RAD - index of accessibility to radial highways
# TAX - full-value property-tax rate per $10,000
# PTRATIO - pupil-teacher ratio by town
# B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
# LSTAT - % lower status of the population
# MEDV - Median value of owner-occupied homes in $1000's

```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>




```python
import plotly.express as px

fig = px.scatter_matrix(house, dimensions = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV'], title='Scatter matrix')
fig.show()

fig = px.scatter_matrix(house, dimensions=['ZN', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'MEDV'], title='Scatter matrix 2')
fig.show()

# LSTAT, INDUS는 MEDV와 지수적 감소 형태를 보임
# RM은 MEDV와 선형 관계를 보임
```

![png](/images/m1/a11/output_3_0.png)
![png](/images/m1/a11/output_3_1.png)



```python
import matplotlib.pyplot as plt

# Create a figure and axes for each variable
fig, axes = plt.subplots(nrows=1, ncols=13, figsize=(20, 5))

# Plot boxplots for each variable
for i, column in enumerate(house.columns):
    axes[i].boxplot(house[column])
    axes[i].set_title(column)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()
```


    
![png](/images/m1/a11/output_4_0.png)
    



```python
# 상관계수의 절댓값이 0.9보다 큰 변수들이 있는지 확인
import numpy as np

col1=['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']
col2=['ZN', 'AGE', 'DIS', 'TAX','PTRATIO', 'MEDV']

cm1 = np.corrcoef(house[col1].values.T)
cm2 = np.corrcoef(house[col2].values.T)
print(cm1)
print(cm2)
```

    [[ 1.          0.60379972  0.59087892 -0.61380827 -0.73766273]
     [ 0.60379972  1.          0.76365145 -0.39167585 -0.48372516]
     [ 0.59087892  0.76365145  1.         -0.30218819 -0.42732077]
     [-0.61380827 -0.39167585 -0.30218819  1.          0.69535995]
     [-0.73766273 -0.48372516 -0.42732077  0.69535995  1.        ]]
    [[ 1.         -0.56953734  0.66440822 -0.31456332 -0.39167855  0.36044534]
     [-0.56953734  1.         -0.74788054  0.50645559  0.26151501 -0.37695457]
     [ 0.66440822 -0.74788054  1.         -0.53443158 -0.23247054  0.24992873]
     [-0.31456332  0.50645559 -0.53443158  1.          0.46085304 -0.46853593]
     [-0.39167855  0.26151501 -0.23247054  0.46085304  1.         -0.50778669]
     [ 0.36044534 -0.37695457  0.24992873 -0.46853593 -0.50778669  1.        ]]



```python
# LSTAT, INDUS가 y(MEDV)와 지수적 관계를 보이므로 로그를 취해 변환된 특성변수를 만든다
import numpy as np
house['L_LSTAT'] = np.log(house['LSTAT'])
house['L_INDUS'] = np.log(house['INDUS'])
house.head()
```




<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
      <th>L_LSTAT</th>
      <th>L_INDUS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
      <td>1.605430</td>
      <td>0.837248</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
      <td>2.212660</td>
      <td>1.955860</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
      <td>1.393766</td>
      <td>1.955860</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
      <td>1.078410</td>
      <td>0.779325</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
      <td>1.673351</td>
      <td>0.779325</td>
    </tr>
  </tbody>
</table>
</div>




```python
# split X and y
y = house['MEDV'].values
house = house.drop(['LSTAT', 'INDUS', 'MEDV'], axis=1)
X = house.values

# split train and test 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

```

## 1. 선형회귀분석


```python
# create a model
from sklearn.linear_model import LinearRegression

mlr = LinearRegression()
mlr.fit(X_train, y_train)

# print('Intercept: ', mlr.intercept_)
# 회귀계수 출력
mlr_coef = mlr.coef_
print(mlr_coef)
```

    [-1.36676828e-01  3.13177997e-02  2.52393199e+00 -1.70295629e+01
      1.23977704e+00  3.06818458e-02 -1.28840466e+00  2.61968148e-01
     -6.58141653e-03 -8.27862485e-01  4.90558897e-03 -9.97211822e+00
     -6.04522425e-01]



```python
# apply the model(predict y) on train&test sets to draw residual plot
y_train_pred = mlr.predict(X_train)
y_test_pred = mlr.predict(X_test)

# draw residual plot
import plotly.graph_objects as go

fig = go.Figure()
fig.add_trace(go.Scatter(x = y_train_pred, y = (y_train_pred - y_train), mode = 'markers', name = 'Training data'))
fig.add_trace(go.Scatter(x = y_test_pred, y = (y_test_pred - y_test), mode = 'markers', name = 'Test data'))
fig.update_xaxes(title_text='predicted')
fig.update_yaxes(title_text='residuals')
fig.update_layout(width=600, height=400, title_text='Residual Plots versus predicted values', title_x=0.5)
fig.show()
```

![png](/images/m1/a11/output_10_0.png)



```python
# MSE(Mean Squared Error)
from sklearn.metrics import mean_squared_error
print('MSE train: %.3f, MSE test: %.3f' % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)) )
```

    MSE train: 18.139, MSE test: 17.416



```python
# R^2
from sklearn.metrics import r2_score
print('R^2 train: %.3f, R^2 test: %.3f' % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))
```

    R^2 train: 0.777, R^2 test: 0.810


## 2. robust - RANSAC


```python
from sklearn.linear_model import RANSACRegressor

rans = RANSACRegressor(max_trials=100, min_samples=45, loss='absolute_error', residual_threshold=5.0, random_state=1)
rans.fit(X, y)

y_train_pred = rans.predict(X_train)
y_test_pred = rans.predict(X_test)
```


```python
# RANSAC 회귀계수 출력
rans_coef = rans.estimator_.coef_
print(rans_coef)
```

    [-8.21729543e-02  4.79672904e-02  1.86320455e+00 -1.08010880e+01
      2.71899072e+00  4.87434792e-04 -1.23059494e+00  2.33657697e-01
     -1.22754421e-02 -6.51276042e-01  1.01945159e-02 -6.40524327e+00
     -9.66003873e-01]



```python
# get inliers and outliers
inlier_mask = rans.inlier_mask_
#  inlier_mask # inlier를 True로 출력한다 
# np.logical_not(inlier_mask) # outlier를 True로 출력한다 
```

## 3. SVM 회귀
### 3.1 선형 SVM 회귀



```python
from sklearn.svm import SVR

# linear SVM regression
svl = SVR(kernel='linear', C=1.0, epsilon=0.1)
svl.fit(X_train, y_train)

y_train_pred_svl = svl.predict(X_train)
y_test_pred_svl = svl.predict(X_test)
```


```python
# 선형 SVM 회귀계수 출력

#  print(svl.support_)
svl_coef = svl.coef_
print(svl_coef)
# print(svl.intercept_)
```

    [[-0.10458325  0.02277776  1.6622706  -2.2907642   3.54910712 -0.01210731
      -0.76011452  0.17583502 -0.01029967 -0.64100847  0.01212295 -5.77934948
      -0.60736674]]


### 3.2 비선형 SVM 회귀
```python
# non-linear SVM regression 
svr = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')
svr.fit(X_train, y_train)

y_train_pred_svr = svr.predict(X_train)
y_test_pred_svr = svr.predict(X_test)
```


```python
# MSE
from sklearn.metrics import mean_squared_error

mse_l_train=mean_squared_error(y_train, y_train_pred_svl).round(3)
mse_l_test=mean_squared_error(y_test, y_test_pred_svl).round(3)
mse_n_train=mean_squared_error(y_train, y_train_pred_svr).round(3)
mse_n_test=mean_squared_error(y_test, y_test_pred_svr).round(3)

print('linear (train test):', mse_l_train, mse_l_test, '\nrbf (train test): ', mse_n_train, mse_n_test)
# MSE를 기준으로 선형svm회귀가 비선형 svm 회귀보다 우수하다.
# 복잡한 모형이 항상 좋다는 것은 편견에 불과함
```

    linear (train test): 22.27 16.382 
    rbf (train test):  66.444 75.256



```python
# R^2
# 단, r-square 값은 선형에서만 의미있고 비선형에서는 의미 없음 
from sklearn.metrics import r2_score

r2_l = r2_score(y_train, y_train_pred_svl)
print(r2_l)
```

    0.72570037073149


## 4. 회귀계수 비교


RANSAC 회귀는 outlier가 제거된 consensus set에 적합하는 회귀이므로 outlier의 영향을 크게 받지 않는다. RANSAC은 이상치가 제거된 OLS 선형모형이라 할 수 있으므로, RANSAC 회귀계수와 linear regression 및 선형 SVM의 회귀계수를 비교함으로써 어떤 특성변수가 outlier의 영향을 크게 받았는지 확인할 수 있다.

1\) RANSAC 회귀계수와 linear regression 회귀계수 비교
- RANSAC과 linear regression의 회귀계수를 비교하면, 네 번째 특성변수와 열 두번째 특성변수가 다른 특성변수 대비 큰 차이를 보인다. 이상치의 영향을 크게 받았음을 짐작할 수 있다. 

2\) RANSAC 회귀계수와 선형 SVM 회귀계수 비교
- RANSAC과 선형 SVM 회귀의 회귀계수를 비교하면, 앞선 1\)과 유사하게 네 번째 특성변수가 다른 특성변수 대비 큰 차이를 보여 이상치의 영향을 크게 받았음을 짐작할 수 있다. 

3\) RANSAC-SVM의 회귀계수 차이가 RANSAC-linear의 회귀계수 차이보다 전반적으로 작으므로, 선형 SVM 회귀가 linear regression보다 outlier에 대한 저항성이 크다고 할 수 있다. 


```python
svl_coef = svl_coef.reshape(13,)
rans_mlr = rans_coef - mlr_coef
rans_svl = rans_coef - svl_coef

print('RANSAC coef: \n',rans_coef.round(1))
print('linear regression coef: \n', mlr_coef.round(1))
print('\nAbsolute difference between RANSAC coef and linear regression coef: \n', np.abs(rans_mlr).round(2))
print('sum: ', np.sum(np.abs(rans_mlr)).round(2))

print('\nRANSAC coef: \n', rans_coef.round(1))
print('linear SVM coef: \n',svl_coef.round(1))
print('\nAbsolute difference between RANSAC coef and linear SVM regression coef: \n', np.abs(rans_svl).round(2))
print('sum: ', np.sum(np.abs(rans_svl)).round(2))

print('\nRANSAC과 선형SVM의 차이가 RANSAC과 선형회귀의 차이보다 작은가?\n', rans_svl<rans_mlr)
```

    RANSAC coef: 
     [ -0.1   0.    1.9 -10.8   2.7   0.   -1.2   0.2  -0.   -0.7   0.   -6.4
      -1. ]
    linear regression coef: 
     [ -0.1   0.    2.5 -17.    1.2   0.   -1.3   0.3  -0.   -0.8   0.  -10.
      -0.6]
    
    Absolute difference between RANSAC coef and linear regression coef: 
     [0.05 0.02 0.66 6.23 1.48 0.03 0.06 0.03 0.01 0.18 0.01 3.57 0.36]
    sum:  12.67
    
    RANSAC coef: 
     [ -0.1   0.    1.9 -10.8   2.7   0.   -1.2   0.2  -0.   -0.7   0.   -6.4
      -1. ]
    linear SVM coef: 
     [-0.1  0.   1.7 -2.3  3.5 -0.  -0.8  0.2 -0.  -0.6  0.  -5.8 -0.6]
    
    Absolute difference between RANSAC coef and linear SVM regression coef: 
     [0.02 0.03 0.2  8.51 0.83 0.01 0.47 0.06 0.   0.01 0.   0.63 0.36]
    sum:  11.13
    
    RANSAC과 선형SVM의 차이가 RANSAC과 선형회귀의 차이보다 작은가?
     [ True False False  True  True False  True False False  True  True  True
     False]


# 2. house 데이터에 적용된 SVM에서 epsilon의 역할
- epsilon은 튜브의 너비를 결정한다. 2×ε이 튜브의 너비이며, 이 안에 위치하는 관측치는 회귀선을 결정하는 계산에 영향을 미치지 못한다. 계산에 기여하는 것은 튜브의 양 경계선에 위치하거나 그보다 멀리 떨어진 관측치이며, 이것을 서포트벡터라고 칭한다. 
- SVM회귀에서 epsilon이 커지면 튜브가 점차 많은 영역을 커버하므로 서포트벡터의 수가 줄어들어 회귀계수 추정에 기여하는 관측치가 줄어든다. 그러므로 지나치게 큰 epsilon은 과소적합(underfit)의 원인이 된다. 반대로, epsilon이 작아지면 튜브의 너비가 좁아지므로 서포트벡터의 수가 증가한다. 이는 모델이 training data에 과대적합(overfit) 되는 원인이 되므로 적절한 epsilon 값을 찾는 것이 중요하다. 



