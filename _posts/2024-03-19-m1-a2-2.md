---
layout: single
title: ""
categories: ML
sidebar: true
use_math: true
---
## 문제
특성변수의 선택에서 교재에 있는 digit 데이터를 이용하여 목적변수가 분류일 때 chi2, f_classif, mutual_info_classif를 적용한 결과 비교
---
- 8\*8 픽셀로 표현된 손글씨이므로 **중앙에 위치한 변수**가 선택되는가를 보아야 한다. 가장자리에는 글자가 없으므로 선택되지 않는 것이 일반적이다. 
- **chi2와 F**는 통계적으로 **같은 성격**을 가지고 있기 때문에(large sample에서는 F가 chi2로 수렴하기 때문임) 실제로 이러한 경향이 있는지 살펴보아야 한다. 
- 변수선택은 데이터의 사전처리과정(pre-processing)이다. 변수선택이 잘 되었는지 여부를 모형에 적합시켜 판단하지 **말아야** 한다. 
    - 선형모형에 적용하면 F나 chi2가 당연히 좋을 것이고, 비선형모형에 적용하면 mutual_info가 좋게 나오게 된다. 데이터의 사전정리는 모형과 관계없이 실행되어야 한다.


```python
from sklearn.datasets import load_digits
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif

x, y = load_digits(return_X_y=True)
x.shape
```




    (1797, 64)




```python
import numpy as np
print(x[ :2])
print(np.unique(y))

```

    [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.
      15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.
       0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.
       0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]
     [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.
       3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.
      16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.
       0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]]
    [0 1 2 3 4 5 6 7 8 9]


1\) SelectKBest 함수의 score_func 파라미터에 따라 64개의 특성변수 중 무엇이 선택되었는지 살펴본다(선택하는 특성변수의 개수는 20개로 동일). 아래 그림에서 흰색은 선택된 특성변수, 검은색은 선택되지 않은 특성변수이다. digits 데이터는 8*8 픽셀로 표현된 손글씨 이미지이므로 가장자리보다는 중앙에 위치한 변수가 선택되어야 하는데, 세 개의 score function 모두 중앙부에 자리한 특성변수를 잘 선택하고 있다. 

|chi2|f_classif|mutual_info_classif|
|-----|---------|--------------------|
|![image.png](/images/m1/a2_2/image.png)|![image-2.png](/images/m1/a2_2/image-2.png)|![image-3.png](/images/m1/a2_2/image-3.png)|

2\) F 통계량은 χ^2통계량의 비율로 정의되므로 통계적으로 같은 성격을 지녀 특성변수 선택 결과 또한 chi2와 f_classif가 유사할 것으로 예상하였다. 그러나 본 데이터셋에서는 f_classif와 mutual_info_classif가 64개 특성변수 중 60개에 대해 동일한 결과(선택/드롭)를 보여주었으며, 위의 매트릭스 플롯도 시각적으로 유사하다. 한편 chi2와 f_classif는 64개 특성변수 중 54개에 대해 동일한 결과를 나타내었다.

-----

### 1) score_func = chi2


```python
sb_chi2 = SelectKBest(score_func=chi2, k=20)
x_new = sb_chi2.fit_transform(x, y)
x_new.shape
```




    (1797, 20)




```python
sb_chi2.get_support() # 64개의 특성변수 중 선택된 것은 True로 표기
# sb.scores_ # 각 특성변수의 카이제곱 값 
# sb.pvalues_ # 각 특성변수의 p-value 
```




    array([False, False, False, False, False,  True,  True, False, False,
           False, False, False, False,  True, False, False, False, False,
           False,  True,  True,  True, False, False, False, False,  True,
           False,  True, False,  True, False, False,  True,  True, False,
           False, False, False, False, False,  True,  True,  True,  True,
           False,  True, False, False, False, False, False, False, False,
            True, False, False, False,  True, False, False,  True,  True,
           False])




```python
mat = sb_chi2.get_support().reshape(8,8)

import matplotlib.pyplot as plt
plt.gray()
plt.matshow(mat)
plt.show()
```


    <Figure size 640x480 with 0 Axes>



    
![png](/images/m1/a2_2/output_7_1.png)
    


<span style="color:red">중앙에 위치한 변수들이 잘 선택되었는가?</span>
- 음.. 그래 보인다


```python
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
import pandas as pd
import plotly.express as px

md1 = make_pipeline(sb_chi2, PCA(n_components=3))
x_new = md1.fit_transform(x,y)
```


```python
x_new
```




    array([[  4.53629534,  -3.32327754, -17.48797087],
           [ -3.13375022,  -3.96493923,  15.94393059],
           [ -2.05715337,   0.89457421,   6.25510885],
           ...,
           [  5.2426066 ,  -5.42920852,  13.82807493],
           [ -5.25425777,   2.46289342, -12.82610544],
           [  3.25870376,   8.40725494,   4.09841042]])




```python
df1 = pd.DataFrame({'X': x_new[:, 0],
                    'Y': x_new[:, 1],
                    'Z': x_new[:, 2],
                    'C': y})
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X</th>
      <th>Y</th>
      <th>Z</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.536295</td>
      <td>-3.323278</td>
      <td>-17.487971</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-3.133750</td>
      <td>-3.964939</td>
      <td>15.943931</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-2.057153</td>
      <td>0.894574</td>
      <td>6.255109</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-8.032520</td>
      <td>14.481054</td>
      <td>3.317395</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.399815</td>
      <td>-3.613571</td>
      <td>8.021796</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
fig = px.scatter_3d(df1, x='X', y='Y', z='Z', color='C')
fig.show()
```
![png](/images/m1/a2_2/newplot0.png)

<span style='color:red'>선형모형인 PCA에 적용했을 때</span>
- (이론 상) chi2와 F가 mutual_info보다 좋게 나온다.
  - 카이제곱과 F는 특성변수와 목적변수가 선형관계일 때 잘 적용된다고 함
  - 그런데 위 플롯을 보면.. 정말 그런지는 잘 모르겠다.
- 단, 변수가 잘 선택되었는지 여부를 모형에 적합시켜 판단하면 **안된다** 



```python
from sklearn.decomposition import KernelPCA
md2 = make_pipeline(sb_chi2, KernelPCA(kernel='rbf', n_components=3))
x_new2 = md2.fit_transform(x,y)
df2 = pd.DataFrame({'X': x_new2[ : ,0],
                    'Y': x_new2[ : ,1],
                    'Z': x_new2[ : ,2],
                    'C': y})
```


```python
fig=px.scatter_3d(df2, x='X', y='Y', z='Z', color='C')
fig.show()
# 숫자 1과 숫자 6이 잘 구분됨을 확인 
```
![png](/images/m1/a2_2/newplot1.png)

<span style='color:red'>**비**선형모형인 Kernel PCA에 적용했을 때</span>
- (이론 상) chi2와 F가 mutual_info보다 **안좋게** 나온다.
    - 0과 1 사이에 2와 4가 섞여 있는 등, 구별이 잘 되는 편은 아닌 듯
- 단, 변수가 잘 선택되었는지 여부를 모형에 적합시켜 판단하면 **안된다** 

### 2) f_classif


```python
sb_f = SelectKBest(score_func=f_classif, k=20)
x_new = sb_f.fit_transform(x, y)

sb_f.get_support() # 64개의 특성변수 중 선택된 것은 True로 표기
mat = sb_f.get_support().reshape(8,8)

plt.gray()
plt.matshow(mat)
plt.show()
```

    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning:
    
    Features [ 0 32 39] are constant.
    
    /Users/ykgoh/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning:
    
    invalid value encountered in divide
    



    <Figure size 640x480 with 0 Axes>



    
![png](/images/m1/a2_2/output_18_2.png)
    


<span style="color:red">중앙에 위치한 변수들이 잘 선택되었는가?</span>
- 그런 것 같다!


```python
md1 = make_pipeline(sb_f, PCA(n_components=3))
x_new = md1.fit_transform(x,y)

df1 = pd.DataFrame({'X': x_new[:, 0],
                    'Y': x_new[:, 1],
                    'Z': x_new[:, 2],
                    'C': y})

fig = px.scatter_3d(df1, x='X', y='Y', z='Z', color='C')
fig.show()
```
![png](/images/m1/a2_2/newplot2.png)

<span style='color:red'>선형모형인 PCA에 적용했을 때</span>
- (이론 상) chi2와 F가 mutual_info보다 좋게 나온다.
    - 그런데 위 플롯을 보면.. 정말 그런지는 잘 모르겠다.
- 단, 변수가 잘 선택되었는지 여부를 모형에 적합시켜 판단하면 **안된다** 


```python
md2 = make_pipeline(sb_f, KernelPCA(kernel='rbf', n_components=3))
x_new2 = md2.fit_transform(x,y)
df2 = pd.DataFrame({'X': x_new2[ : ,0],
                    'Y': x_new2[ : ,1],
                    'Z': x_new2[ : ,2],
                    'C': y})
fig=px.scatter_3d(df2, x='X', y='Y', z='Z', color='C')
fig.show()
```
![png](/images/m1/a2_2/newplot3.png)

<span style='color:red'>**비**선형모형인 Kernel PCA에 적용했을 때</span>
- (이론 상) chi2와 F가 mutual_info보다 **안좋게** 나온다.
    - 그런데 위 플롯을 보면.. 정말 그런지는 잘 모르겠다. 잘 나오는 것 같은데?
    - 구별이 어려운 지점이 있음(마커가 모인 곳)
- 단, 변수가 잘 선택되었는지 여부를 모형에 적합시켜 판단하면 **안된다** 

### 3) mutual_info_classif
> sklearn.feature_selection.mutual_info_classif(X, y, \*, discrete_features='auto', n_neighbors=3, copy=True, random_state=None

Mutual information(MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.

The function relies on *nonparametric* methods based on **entropy estimation from k-nearest neighbors distances.** 

It can be used for univariate features selection


```python
sb_mi = SelectKBest(score_func=mutual_info_classif, k=20)
x_new = sb_mi.fit_transform(x, y)

sb_mi.get_support() # 64개의 특성변수 중 선택된 것은 True로 표기
mat = sb_mi.get_support().reshape(8,8)

plt.gray()
plt.matshow(mat)
plt.show()
```


    <Figure size 640x480 with 0 Axes>



    
![png](/images/m1/a2_2/output_25_1.png)
    


<span style="color:red">중앙에 위치한 변수들이 잘 선택되었는가?</span>
- 그런 것 같다!


```python
md1 = make_pipeline(sb_mi, PCA(n_components=3))
x_new = md1.fit_transform(x,y)

df1 = pd.DataFrame({'X': x_new[:, 0],
                    'Y': x_new[:, 1],
                    'Z': x_new[:, 2],
                    'C': y})

fig = px.scatter_3d(df1, x='X', y='Y', z='Z', color='C')
fig.show()
```
![png](/images/m1/a2_2/newplot4.png)

<span style='color:red'>선형모형인 PCA에 적용했을 때</span>
- (이론 상) mutual_info가 chi2와 F보다 안좋게 나온다. 
    - 근데 이것두 잘 나온 것 같은데?
- 단, 변수가 잘 선택되었는지 여부를 모형에 적합시켜 판단하면 **안된다** 


```python
md2 = make_pipeline(sb_mi, KernelPCA(kernel='rbf', n_components=3))
x_new2 = md2.fit_transform(x,y)
df2 = pd.DataFrame({'X': x_new2[ : ,0],
                    'Y': x_new2[ : ,1],
                    'Z': x_new2[ : ,2],
                    'C': y})
fig=px.scatter_3d(df2, x='X', y='Y', z='Z', color='C')
fig.show()
```
![png](/images/m1/a2_2/newplot5.png)

<span style='color:red'>**비**선형모형인 Kernel PCA에 적용했을 때</span>
- (이론 상) mutual_info가 chi2나 F보다 **좋게** 나온다.
    - 이거 보고 chi2, F의 비선형 커널PCA 플롯을 보면 확실히 이게 깔끔하긴 함
    - 0, 1, 5, 6이 잘 구분됨
- 단, 변수가 잘 선택되었는지 여부를 모형에 적합시켜 판단하면 **안된다** 


```python
arr_chi2 = sb_chi2.get_support()
arr_f = sb_f.get_support()
arr_mi = sb_mi.get_support()

chi2_f = (arr_chi2 == arr_f).sum()
f_mi = (arr_f == arr_mi).sum()
chi2_mi = (arr_chi2 == arr_mi).sum()
print('chi2_f: {}, f_mi: {}, chi2_mi: {}'.format(chi2_f, f_mi, chi2_mi))

```

    chi2_f: 54, f_mi: 60, chi2_mi: 54

