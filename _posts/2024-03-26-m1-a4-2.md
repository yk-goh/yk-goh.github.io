# chapter 4. 연습문제 2
#### iris 데이터에 Kernel Density 함수를 적용하되 bandwidth를 조절하여 그 효과를 논의하고 'gaussian' 이외의 kernel을 적용하여 차이를 비교
---
- bandwidth가 커질수록 smoothing이 강해진다. 이는 histogram의 width와 같은 원리이다. <br>
bandwidth가 너무 작으면 자료분포에 대한 정보가 없고, 너무 크면 분포가 뭉개지므로 이 또한 문제다.
- 일반적으로 epanechnikov나 gaussian kernel을 사용하는 것이 자료분포를 파악하는 데 좋은 것으로 알려져 있다.
- KDE(Kernel Density Estimation)에 사용하는 커널의 비교는 kernel weight의 형태에 따라 설명해야 한다.
![available kernels](https://scikit-learn.org/stable/_images/sphx_glr_plot_kde_1d_002.png)

참고: [scikit-learn Density Estimation](https://scikit-learn.org/stable/modules/density.html)

1\) 먼저, iris 데이터의 특성변수 중 하나인 ‘sepal_length’의 분포를 확인한다. ‘sepal_length’의 값이 4.3과 7.9 사이에 분포하며, 중앙값이 5.8이고 5.1과 6.4 사이에 50%의 관측치가 분포함을 알 수 있다.


```python
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

iris = sns.load_dataset('iris')
X = iris.drop('species', axis=1)
xx = X['sepal_length'].values
xx = xx.reshape(-1,1)

# 특성변수인 sepal_length 의 분포 범위 확인 
iris['sepal_length'].hist()
iris['sepal_length'].describe()
```




    count    150.000000
    mean       5.843333
    std        0.828066
    min        4.300000
    25%        5.100000
    50%        5.800000
    75%        6.400000
    max        7.900000
    Name: sepal_length, dtype: float64




    
![png](output_2_1.png)
    


2\) 커널함수는 하나의 관측치로부터의 거리에 비례하여 다른 관측치들에 가중치를 부여하는 함수이다. 표본(관측치) 각각이 커널함수가 적용된 분포를 갖게 되는데, 이를 선형결합(커널함수값을 모두 더하고 표본수로 나눈다) 하여 커널분포함수 추정치를 얻는다. 커널분포함수를 추정할 때 (1) 가중치의 비중을 어떻게 조정할지 (2) 어떤 커널함수를 사용할지가 관건이 된다.

2\)-(1) bandwidth의 효과
KernelDensity() 함수의 파라미터인 bandwidth는 가중치의 비중을 조절한다. bandwidth가 작을수록 가까운 관측치에 더 큰 가중치를 준다. 교재에서는 해당 파라미터에 0.2의 값을 부여했고, 함수에 설정된 default 값은 1.0이므로 bandwidth를 [0.2, 0.6, 1.0, 1.4, 1.8]로 조정하며 플롯을 그려 iris 데이터셋의 'sepal_length' 변수의 커널분포함수를 추정한다. 
-	bandwidth = 0.2일 때, 극대값(local maximum)이 여럿 존재하는 모자 형태이다. smoothing 효과가 크지 않아, 위에서 살펴본 히스토그램과 유사한 모양으로 나타난다.
-	bandwidth = 0.6일 때, 확률을 추정하기 위해 상대적으로 멀리 떨어진 관측치까지 고려하므로 bandwidth=0.2일 때보다 모양이 부드러워졌다.
-	bandwidth가 커질수록 smoothing 효과가 강해, 꼬리가 두꺼워지면서 분포가 평평해진다. 분포가 뭉개져 자료분포에 대한 정보를 얻기 어렵다. 



```python
from sklearn.neighbors import KernelDensity


fig, axes = plt.subplots(1,5, figsize=(12,3), sharey=True)
fig.subplots_adjust(hspace=1.2, wspace=.1)
bw_list = np.arange(start=.2, stop=2, step=.4).round(1)

for ax, bw in zip(axes.flat, bw_list):
    kd = KernelDensity(bandwidth=bw, kernel='gaussian') # bandwidth 기본값은 1.0
    kd.fit(xx)
    xx_d = np.linspace(3,9,1000)
    # score_samples는 log-likelihood를 계산한다
    logprob = kd.score_samples(xx_d.reshape(-1,1))
    ax.fill_between(xx_d, np.exp(logprob), color='lightcoral')
    ax.set_title('bandwidth={}'.format(bw), {'fontsize':10})
    
```


    
![png](output_4_0.png)
    


2\)-(2) 다양한 커널을 적용했을 때 차이 비교

커널분포함수 추정치는 각 데이터포인트에 커널함수를 적용한 뒤 선형결합하여 구하게 되므로, 적용하는 커널함수의 분포 모양에 따라 커널분포함수 추정치 또한 다른 모양을 띈다. 

bandwidth = .2일 때,

- epanechnikov, linear, cosine 커널함수는 (-bandwidth, +bandwidth) 구간에서 확률값을 갖고, 따라서 중심에서의 최댓값이 gaussian 커널함수 대비 크기 때문에 이를 적용한 커널분포함수 추정치는 뾰족한 극소점과 극대점을 여럿 갖는다
- gaussian 커널함수를 적용했을 때에도 커널분포함수 추정치에 극점이 나타나지만 위의 경우보다는 완만하다. gaussian 분포의 구간은 (-∞, +∞)이므로 위의 epanechnikov, linear, cosine  커널함수보다 완만한 분포를 그리기 때문이다. 
- tophat 커널함수는 (-bandwidth, +bandwidth) 구간에서 균등한 분포를 보이므로, 커널분포함수 추정치 또한 계단 모양으로 나타난다.


한편 bandwidth = 1.0일 때,
-	gaussian과 exponential 커널함수를 적용했을 때의 커널분포함수 추정치가 유사한 모양을 보인다.
-	epanechnikov, linear, cosine 커널함수는 각 데이터포인트에 가까울수록 가중치를 부여하는 비중이 gaussian 및  exponential보다 높기 때문에(중심부가 뾰족한 모양) 커널분포함수 추정치 또한 데이터가 다수 분포한 위치에서(5.1~6.4; 1\)번 참고\) 값이 높게 나타난다. 
-	tophat 커널함수의 경우 bandwidth가 커질수록 균등분포 값들이 겹치며 계단이 점차 곡선에 가까워진다.



```python
def get_kd_plots(bw_list):
    from sklearn.neighbors import KernelDensity
    for bw in bw_list:
    
        fig, axes = plt.subplots(1,6, figsize=(12,3), sharey=True)
        fig.subplots_adjust(hspace=1.5, wspace=.1)
        kernel_list = ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine']
        fig.suptitle('bandwidth={}'.format(bw))
        
        for kernel, ax in zip(kernel_list, axes.flat):
            kd = KernelDensity(bandwidth=bw, kernel=kernel) # bandwidth 기본값은 1.0
            kd.fit(xx)
            xx_d = np.linspace(3,9,1000)
            # score_samples는 log-likelihood를 계산한다
            logprob = kd.score_samples(xx_d.reshape(-1,1))
            ax.fill_between(xx_d, np.exp(logprob), color='palegreen')
            ax.set_title('{}'.format(kernel), {'fontsize':10})

```


```python
"""
KernelDensity() 메서드에서 bandwidth 기본값이 1.0이므로 [.2, .6, 1.0, 1.4, 1.8]로 리스트를 만든다
"""
bw_list = np.arange(start=.2, stop=2, step=.4).round(1)
bw_list

get_kd_plots(bw_list)
```


    
![png](output_7_0.png)
    



    
![png](output_7_1.png)
    



    
![png](output_7_2.png)
    



    
![png](output_7_3.png)
    



    
![png](output_7_4.png)
    


- gaussian kernel: 평균에서 가장 두껍고 꼬리가 가는 종 모양의 대칭 분포
- tophat: 특정 구간에서 균등한 확률값을 갖는 분포
- epanechnikov: 중심을 기준으로 대칭이고 중심에서 최대값을 가진다. gaussian과 달리 중심을 기준으로 ±bandwidth에서만 확률값을 갖는다. 그러므로 gaussian과 비교했을 때 중심이 갖는 최댓값이 더 크다. 
- exponential: 지수적으로 감소하는 형태를 띈다. 중심에서 멀어질수록 완만하게 감소한다. 종 모양보다 빠르게 감소하는 형태이다
- linear: 중심에서 최댓값을 가지며, ±bandwidth 구간에서 기울기 변화 없이 증가&감소한다
- cosine: 시각적 형태가 epanechnikov 커널함수와 유사하다



```python

```
