# CART에서 criterion을 gini와 entropy를 각각 적용하고 **max_depth**를 변경하여 결과 비교 및 해석
- depth를 증가시키면서 test data의 accuracy와 overfitting을 논의해야 한다
- 의사결정나무는 표준화가 필요하지 않다
- 초모수의 선택과 모형의 선택은 test data의 accuracy를 높이면서 overfitting이 발생하지 않도록 하는 것이다

## 0. preprocessing


```python
import seaborn as sns

iris = sns.load_dataset('iris')
X = iris.drop('species', axis=1)
y = iris['species']
#logit = LogisticRegression

from sklearn.preprocessing import LabelEncoder
classle = LabelEncoder()
y = classle.fit_transform(y.values)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1, stratify=y)
```

## 1. 분류나무 적합 및 정확도 계산

  Decision tree 알고리즘은 불순도의 최소화를 목적으로 한다. 부모 영역의 불순도 값에서 (분할 노드를 x_i로 했을 때) 자식 영역의 불순도 값을 뺀 값을 정보획득(Information Gain, IG)이라 정의하면 x_i가 분류에 유용한 변수일수록 그 IG가 크므로 IG는 분할(partition) 변수를 결정하는 기준이 된다. 분류나무에서는 불순도(Impurity) 측도로 gini index 또는 cross-entropy를 사용할 수 있는데, 성능 차이는 거의 없는 것으로 알려져 있으나 결정 경계에 약간의 차이가 있을 수 있다. 다음은 Iris 데이터에 ‘gini’와 ‘entropy’를 불순도 측도로 하는 분류나무를 적합하고 Accuracy를 계산한 결과이다. 


```python
from sklearn import tree
from sklearn import metrics
from itertools import product

criterion_list = ['gini', 'entropy']
depth_list = [3, 4, 5, 6, 7, 8, 10, 20]
prod = product(criterion_list, depth_list)

for criterion, depth in prod:
    dtc = tree.DecisionTreeClassifier(criterion = criterion, max_depth=depth, random_state=1)
    dtc.fit(X_train, y_train)
    y_train_pred = dtc.predict(X_train)
    y_test_pred = dtc.predict(X_test)

    print('Accuracy when criterion={} and max_depth={}'.format(criterion, depth))
    print('Train: ',metrics.accuracy_score(y_train, y_train_pred).round(2))
    print('Test: ',metrics.accuracy_score(y_test, y_test_pred).round(2))
```

    Accuracy when criterion=gini and max_depth=3
    Train:  0.95
    Test:  0.98
    Accuracy when criterion=gini and max_depth=4
    Train:  0.97
    Test:  0.98
    Accuracy when criterion=gini and max_depth=5
    Train:  0.99
    Test:  0.98
    Accuracy when criterion=gini and max_depth=6
    Train:  1.0
    Test:  0.98
    Accuracy when criterion=gini and max_depth=7
    Train:  1.0
    Test:  0.98
    Accuracy when criterion=gini and max_depth=8
    Train:  1.0
    Test:  0.98
    Accuracy when criterion=gini and max_depth=10
    Train:  1.0
    Test:  0.98
    Accuracy when criterion=gini and max_depth=20
    Train:  1.0
    Test:  0.98
    Accuracy when criterion=entropy and max_depth=3
    Train:  0.95
    Test:  0.96
    Accuracy when criterion=entropy and max_depth=4
    Train:  0.95
    Test:  0.93
    Accuracy when criterion=entropy and max_depth=5
    Train:  0.97
    Test:  0.98
    Accuracy when criterion=entropy and max_depth=6
    Train:  0.98
    Test:  0.98
    Accuracy when criterion=entropy and max_depth=7
    Train:  0.98
    Test:  0.98
    Accuracy when criterion=entropy and max_depth=8
    Train:  0.99
    Test:  0.98
    Accuracy when criterion=entropy and max_depth=10
    Train:  1.0
    Test:  0.98
    Accuracy when criterion=entropy and max_depth=20
    Train:  1.0
    Test:  0.98


불순도 측도로 gini를 적용하고 max_depth=3으로 지정했을 때, train set에서의 accuracy가 상대적으로 낮게(0.95) 나와 max_depth를 단계적으로 증가시켰다. max_depth가 5 이상일 때, test set에서의 accuracy가 train set의 accuracy 보다 낮아, overfitting을 의심할 수 있다. 

한편, 불순도 측도로 entropy를 적용하고 max_depth를 3부터 증가시키며 accuracy를 확인하면 max_depth가 8 이상일 때 test set에서의 accuracy가 train set의 accuracy 보다 낮아, overfitting이 의심된다.

CART는 각 셀에 오직 한 개의 학습데이터만 존재할 때까지 영역을 분할 할 수 있으므로 overfitting 문제가 발생하기 쉽다. 그러므로 분할의 깊이(depth)를 제한할 필요가 있으며, 이를 가지치기(pruning)라고 한다. 

