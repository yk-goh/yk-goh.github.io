# Tree-path dependent method
$x_1=Age, x_2=RAD, x_3=TAX, x_4=DIS$일 때 
$E(f(x_1, x_2, x_3, x_4 \mid DIS = 4.09))$의 추정치 $f_s(x_4)$를 의사결정나무를 통해 tree_path_dependent 방법으로 구한다.


```python
from sklearn.tree import DecisionTreeRegressor, plot_tree
import matplotlib.pyplot as plt 
import pandas as pd 

df = pd.read_csv('Boston.csv', index_col=0)
X = df[['AGE', 'RAD', 'TAX', 'DIS']]
y = df['PRICE']

tree_model = DecisionTreeRegressor(max_depth=3)
tree_model.fit(X, y)
fig = plt.figure(figsize=(25,10))
plot_tree(tree_model, feature_names=X.columns, fontsize=16)
plt.show()
```


    
![png](output_1_0.png)
    



`root`

TAX가 주어지지 않았으므로 루트에서 $\frac{340}{506}$의 확률로 왼쪽으로 이동하고 $\frac{166}{506}$의 확률로 오른쪽으로 이동한다.


`root --left--> depth 1`

역시 TAX가 주어지지 않았으므로 depth 1에서 $\frac{92}{340}$의 확률로 왼쪽으로 이동하고 $\frac{248}{340}$의 확률로 오른쪽으로 이동한다.


`root --left--> depth 1 --left--> depth 2`

TAX가 주어지지 않았으므로 왼쪽과 오른쪽 리프로 각각 $\frac{76}{92}$, $\frac{16}{92}$의 확률로 이동한다. 따라서 
$$\frac{340}{506} \times \frac{92}{340} \times (\frac{76}{92} \times 28.378 + \frac{16}{92} \times 37.381) = 5.444$$


`root --left--> depth 1 --right--> depth 2`

다음으로, depth 1에서 오른쪽으로 이동하는 경우를 살펴본다. 
RAD가 주어지지 않았으므로 왼쪽과 오른쪽 리프로 각각 $\frac{224}{248}$, $\frac{24}{248}$의 확률로 이동한다.
$$\frac{340}{506} \times \frac{248}{340} \times (\frac{224}{248} \times 23.028 + \frac{24}{248} \times 30.358) = 11.634$$

이제 루트에서 오른쪽으로 이동한 경우를 살펴보자. <br>

`root --right--> depth 1`

$DIS=4.09$로 주어졌으므로 노드 $DIS<=1.357$이 False이므로 확률 1로 오른쪽 노드로 이동한다. 


`root --right--> depth 1 --right--> depth 2`

$DIS=4.09$로 주어졌으므로 depth 2 노드 $DIS<=2.102$가 False이므로 확률 1로 오른쪽 리프로 이동한다. 그러므로 
$$\frac{166}{506} \times 1 \times 1 \times 18.724 = 6.143$$


최종적으로 위의 값을 모두 더하여 $$f_s(x_4)=5.444+11.634+6.143=23.221$$으로 $E(f(x_1,x_2,x_3,x_4 \mid DIS=4.09))$를 추정하게 된다

![image.png](image.png)
