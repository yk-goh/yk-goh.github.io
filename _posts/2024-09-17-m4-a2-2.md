---
layout: single
title: "Using Anchor method on an image"
categories: XAI
sidebar: true
use_math: true
---


# 문제: 관심있는 사진에 대하여 anchors 구하기


```python
import tensorflow as tf 
import matplotlib 
%matplotlib inline
import matplotlib.pyplot as plt 
import numpy as np 
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions 
from alibi.explainers import AnchorImage
from PIL import Image
```

## 1. 이미지 불러오기
다음 이미지는 고양이 체리가 주방 오븐토스터 위에 앉아 있는 사진이다. Keras 공식문서에 의하면 InceptionV3 인스턴스화 시 include_top 파라미터를 True로 지정할 경우, input shape은 (299, 299, 3)이어야 한다. 때문에 이미지 크기를 (299, 299)로 resize 하였다. 



```python
img = Image.open('cherry3.jpeg')
image = img.resize((299, 299))
image = tf.convert_to_tensor(image)
print(image)

plt.imshow(image)

```

    tf.Tensor(
    [[[173 133 107]
      [171 132 105]
      [169 132 105]
      ...
      [192 189 184]
      [192 188 186]
      [194 189 186]]
    
     [[171 131 105]
      [172 132 106]
      [169 132 105]
      ...
      [191 188 183]
      [192 188 185]
      [194 189 186]]
    
     [[169 129 103]
      [170 130 104]
      [171 131 105]
      ...
      [193 189 185]
      [194 189 186]
      [193 188 185]]
    
     ...
    
     [[ 77  64  57]
      [ 84  70  62]
      [ 84  73  65]
      ...
      [196 179 153]
      [197 180 154]
      [195 178 152]]
    
     [[ 78  63  57]
      [ 85  70  61]
      [ 87  75  66]
      ...
      [196 179 153]
      [198 181 155]
      [197 180 154]]
    
     [[ 78  62  57]
      [ 85  68  60]
      [ 87  74  65]
      ...
      [194 177 151]
      [196 179 153]
      [198 181 155]]], shape=(299, 299, 3), dtype=uint8)





    <matplotlib.image.AxesImage at 0x33a1065e0>




    
![png](/images/m4/a2_2/output_3_2.png)
    


## 2. InceptionV3 모형 통과
- tf.keras에서 이미지 데이터는 4D tensor로 입력해야 하므로 (1, 299, 299, 3)으로 reshape 한다.
- InceptionV3를 v3로 객체화 하고, 함수형 API로 입력층(inputs=input_image)과 출력층(outputs=x)을 지정하여 딥러닝 모델을 정의하였다. 

  tf.keras.Model 클래스 객체의 리턴값이 “Numpy array(s) of predictions”이기 때문에 preds에 Numpy의 함수인 argsort를 사용할 수 있다. argsort는 array를 오름차순으로 정렬하는 인덱스를 반환한다. 아래와 같이 사용하면 preds를 오름차순 정렬 후 밑에서부터(즉 가장 확률이 높은) 항목을 5개 출력한다. 인터넷에서 “imagenet label list”를 검색하면 1000개 클래스에 해당하는 인덱스와 설명을 확인할 수 있다. 입력한 이미지로부터 284번 레이블(siamese cat)이 78.4% 확률로 가장 높게 예측되었고(체리는 귀여운 샴고양이다), 다음으로 811번(space heater)과 859번(toaster)이 각각 8.1%, 1.0% 확률로 예측되었다. 


```python
image = np.reshape(image, (1, 299, 299, 3))

v3 = tf.keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet')

input_image = tf.keras.Input([None, None, 3])
x = input_image
x = tf.keras.applications.inception_v3.preprocess_input(x)
x = v3(x)

model = tf.keras.Model(inputs=input_image, outputs = x)
preds = model.predict(image)

# 오름차순 정렬 후 밑에서부터 5개 꺼내욤: 가장 확률이 높은 5개 레이블 
for item in preds.argsort()[0][-5:]:
    print(item, preds[0, item])
    
# 284 >>>>> 811 space heater > 859 toaster 순으로 확률이 높다 
```

    [1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 847ms/step
    753 0.0053976285
    588 0.0071097505
    859 0.010117765
    811 0.08143277
    284 0.784123



```python
plt.imshow(image[0])
```




    <matplotlib.image.AxesImage at 0x340ab8040>




    
![png](/images/m4/a2_2/output_7_1.png)
    


## 3. anchors 찾기
anchors를 찾기 위한 image_shape을 입력 이미지와 동일한 (299, 299, 3)으로 정의한다. 예측함수로는 람다식으로 정의한 InceptionV3 모델 객체의 predict를 지정한다. segmentation_fn으로는 내장 super-pixel 중 하나인 ‘slic’을 사용한다. 


```python
image_shape = (299, 299, 3)
predict_fn = lambda x: model.predict(x)
explainer = AnchorImage(predictor=predict_fn,
                        image_shape=image_shape,
                        segmentation_fn='slic',
                        images_background=None)
```

    [1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 776ms/step


아래 프로그램을 통해 입력이미지의 super-pixel anchors를 구하여 시각화 한다.
- `threshold`: $P(prec(A) \ge t) \ge 1 - \delta$ 에서 `A`가 Anchor일 때, `t`를 의미한다
- `p_sample`: super-pixel을 임의로 뽑아 1값을 부여하고(on) 뽑히지 않은 super-pixel은 0값을 부여(off)하는 시뮬레이션에서 off 할 super-pixel의 비율을 지정한다
    - 이미지 데이터의 특성변수는 픽셀이므로 블랙박스 모형에 입력되는 특성변수의 수는 매우 많지만, 픽셀 하나가 예측치에 대해 가지는 기여도는 매우 적다. 
    - 이러한 이유로 픽셀의 그룹화인 super-pixel로 유리상자 모형의 합성특성변수를 생성하게 된다. 
    - super-pixel은 이미지 내 객체 간 의미있는 경계선을 찾아 픽셀을 그룹화 한 것이다.
    - 대리모형의 합성특성변수는 super-pixel을 임의로 뽑아 1값을 부여하고 뽑히지 않은 super-pixel은 이미지에서 제거(0값을 부여)한다.
- `tau`: $P(prec(A) \ge prec(A^*) - \tau) \ge 1 - \delta$

아래 그림을 통해 고양이 얼굴을 포함하는 super-pixel이 anchor임을 알 수 있다. 위에서 AnchorImage를 인스턴스화 할 때 images_background=None으로 지정하여 anchor super-pixel를 제외한 부분은 검정색으로 나타나고 있다.  


```python
np.random.seed(0)
explanation = explainer.explain(image[0], threshold=.90, p_sample=.5)
plt.imshow(explanation.anchor)
```

    [1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 59ms/step
    [1m4/4[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m4s[0m 778ms/step
    [1m4/4[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m3s[0m 777ms/step





    <matplotlib.image.AxesImage at 0x3378e1f10>




    
![png](/images/m4/a2_2/output_11_2.png)
    


아래 그림은 segmentation function 중 위에서 사용한 ‘slic’에 의한 super-pixel을 보여주고 있다. 고양이, 오븐토스터, 유리병 등 피사체의 윤곽선을 중심으로 super-pixel이 구성되어 있다. 


```python
plt.imshow(explanation.segments)
```




    <matplotlib.image.AxesImage at 0x33ef045b0>




    
![png](/images/m4/a2_2/output_13_1.png)
    

