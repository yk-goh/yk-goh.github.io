---
layout: single
title: "Hyperparameter `C` in SVM"
categories: ML
sidebar: true
use_math: true
---
# 문제: 선형 SVM에서 c값의 변화에 따른 accuracy의 변화와 과대적합에 미치는 영향
> 선형 SVM에서 c가 커지면 오차를 허용하지 않으므로 분류선의 band가 좁아진다. 이 결과로 support vector의 수는 줄어들고 과대적합이 발생한다


```python
import seaborn as sns

iris = sns.load_dataset('iris')
X = iris.drop('species', axis=1)
y = iris['species']

# 0 1 2 클래스로 인코딩
from sklearn.preprocessing import LabelEncoder
classle = LabelEncoder()
y = classle.fit_transform(y.values)

# train set, test set 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123, stratify=y)

```

iris 데이터에 커널을 ‘linear’로 지정한 SVC 함수를 적합하면서 C 값의 변화에 따라 accuracy가 어떻게 변하는지 확인하였다. 

C=1일 때, 즉 허용하는 오차가 상대적으로 클 때 train set과 test set에서 모두 accuracy가 높게(.99, .98) 나타났다. C 값이 4 이상일 때(허용하는 오차의 크기가 상대적으로 작을 때), train set에서의 accuracy는 .99로 높지만 test set에서의 accuracy는 .93으로 낮아 과대적합을 의심할 수 있다. 
한편 서포트벡터란 유연마진(soft margin) 내부에 존재하는 표본을 의미하므로 C의 값이 커질수록(허용하는 오차의 크기가 작을수록) 밴드의 폭이 좁아져 서포트벡터의 수가 감소함을 확인할 수 있다. C=1일 때 각 클래스의 서포트벡터는 3개, 10개, 8개이지만 C=5일 때 각 클래스의 서포트벡터는 3개, 6개, 4개이다. 



```python

# SVM에는 SVC, SVR이 있는데 분류가 목적이면 SVC를 사용한다. SVR은 회귀 
from sklearn.svm import SVC
from sklearn import metrics

for C in [1, 2, 3, 4, 5, 10, 100]:
    svc = SVC(kernel='linear', C=C, random_state=1) # capital C
    svc = svc.fit(X_train, y_train)
    y_train_pred = svc.predict(X_train)
    y_test_pred = svc.predict(X_test)

    print('\n------------------------- C={} -------------------------'.format(C))
    print('train accuracy: ',metrics.accuracy_score(y_train, y_train_pred).round(2))
    print('test accuracy: ',metrics.accuracy_score(y_test, y_test_pred).round(2))
    # 아래에서 support는 해당 클래스(실제값) 관측치 개수임
    # print(metrics.classification_report(y_test, y_test_pred))
    # 첫번째 클래스인 setosa를 구분하기 위해 3개의 표본을 서포트벡터로 사용,
    # 두번째 클래스인 versicolor을 구분하기 위해 10개의 표본을 서포트벡터로 사용,
    # 세번째 클래스인 virginica를 구분하기 위해 8개의 표본을 서포트벡터로 사용
    print('number of support vectors for each class:', svc.n_support_)
    # print(svc.support_)
```

    
    ------------------------- C=1 -------------------------
    train accuracy:  0.99
    test accuracy:  0.98
    number of support vectors for each class: [ 3 10  8]
    
    ------------------------- C=2 -------------------------
    train accuracy:  0.99
    test accuracy:  0.98
    number of support vectors for each class: [3 8 7]
    
    ------------------------- C=3 -------------------------
    train accuracy:  0.99
    test accuracy:  0.98
    number of support vectors for each class: [3 7 5]
    
    ------------------------- C=4 -------------------------
    train accuracy:  0.99
    test accuracy:  0.93
    number of support vectors for each class: [3 6 5]
    
    ------------------------- C=5 -------------------------
    train accuracy:  0.99
    test accuracy:  0.93
    number of support vectors for each class: [3 6 4]
    
    ------------------------- C=10 -------------------------
    train accuracy:  0.99
    test accuracy:  0.93
    number of support vectors for each class: [3 5 4]
    
    ------------------------- C=100 -------------------------
    train accuracy:  0.99
    test accuracy:  0.93
    number of support vectors for each class: [3 4 3]


서포트벡터머신의 목적은 관측치를 잘 분류하는 직선 $f(X)=\beta_0+\beta^TX$를 찾는 것이다. 그런데 현실에서는 두 그룹이 완전하게 분리되지 않으므로 완화변수(slack variable) ξ를 도입한다. ξ는 soft margin을 만드는 역할을 하며, 허용되는 오차라 할 수 있다. 

허용되는 오차의 크기가 밴드의 너비를 결정한다. 목적함수에서 C는 hyperparameter로서 오차 합계(∑ξ)에 대한 가중치인데, C가 클수록 오차가 조금만 커져도 목적함수(손실함수) 값이 크게 증가하므로 오차를 허용하지 않아 밴드의 너비가 좁아진다. 이는 overfitting 문제가 발생할 가능성이 높아짐을 의미한다. 반대로 C를 아주 작게 주면 오차 합계가 커도 되므로 밴드의 너비가 넓어져 bias가 발생할 수 있다. 아래는 임의로 생성한 데이터에 선형 SVM을 적용하고 C 값을 달리 하여 밴드의 너비를 확인한 결과이다. C가 클수록, 즉 허용되는 오차가 작을수록 밴드의 너비가 좁음을 확인할 수 있다.



```python
import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import make_blobs
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.svm import LinearSVC

X, y = make_blobs(n_samples=50, centers=2, random_state=0)

plt.figure(figsize=(14, 5))
for i, C in enumerate([1, 10, 100]):
    # "hinge" is the standard SVM loss
    clf = svm.LinearSVC(C=C, max_iter=20000).fit(X, y)
    # obtain the support vectors through the decision function
    decision_function = clf.decision_function(X)
    # we can also calculate the decision function manually
    # decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]
    # The support vectors are the samples that lie within the margin
    # boundaries, whose size is conventionally constrained to 1
    support_vector_indices = np.where(np.abs(decision_function) <= 1 + 1e-15)[0]
    support_vectors = X[support_vector_indices]

    plt.subplot(1, 3, i + 1)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Set1)
    ax = plt.gca()
    DecisionBoundaryDisplay.from_estimator(
        clf,
        X,
        ax=ax,
        grid_resolution=50,
        plot_method="contour",
        colors="k",
        levels=[-1, 0, 1],
        alpha=0.5,
        linestyles=["--", "-", "--"],
    )
    plt.scatter(
        support_vectors[:, 0],
        support_vectors[:, 1],
        s=100,
        linewidth=1,
        facecolors="none",
        edgecolors="k",
    )
    plt.title("C=" + str(C))
plt.tight_layout()
plt.show()
```


    
![png](/images/m1/a8_1/output_5_1.png)
    

